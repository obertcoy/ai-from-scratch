{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_z(X, w, b):\n",
    "    \n",
    "    z = jnp.dot(X, w) + b\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_func(z):\n",
    "    \n",
    "    return z\n",
    "\n",
    "def relu_func(z):\n",
    "    \n",
    "    return jnp.maximum(z, 0)\n",
    "\n",
    "def sigmoid_func(z):\n",
    "    \n",
    "    return 1 / (1 + jnp.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss_func(f_wb_i, y_i):\n",
    "    \n",
    "    return (-y_i * jnp.log(f_wb_i)) - ((1 - y_i) * jnp.log(1 - f_wb_i))\n",
    "\n",
    "\n",
    "def compute_cost(X, y, w, b, activation_func, loss_func):\n",
    "    \n",
    "    z = compute_z(X, w, b)\n",
    "    f_wb = activation_func(z)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    cost = jnp.mean(loss_func(f_wb, y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (50, 5)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_z() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m neuron1 \u001b[38;5;241m=\u001b[39m Neuron(test_input)\n\u001b[1;32m---> 30\u001b[0m out1 \u001b[38;5;241m=\u001b[39m \u001b[43mneuron1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[62], line 19\u001b[0m, in \u001b[0;36mNeuron.call\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_func(\u001b[43mcompute_z\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_z() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, weights= -1, bias= -1, activation_func=linear_func, verbose=0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.activation_func = activation_func\n",
    "        \n",
    "        if weights == -1:\n",
    "            weights = np.random.random_sample(X.shape[-1])\n",
    "            \n",
    "        if bias == -1:\n",
    "            bias = np.random.rand()\n",
    "            \n",
    "        self.weights = weights\n",
    "        self.bias = bias        \n",
    "        \n",
    "    def call(self) -> float:\n",
    "        \n",
    "        out = self.activation_func(compute_z(self.X, self.weights) + self.bias)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    \n",
    "test = np.zeros(5)\n",
    "test_input = np.random.random_sample((50,5))\n",
    "# print(f'Input: {test_input}')\n",
    "print(f'Shape: {test_input.shape}')\n",
    "\n",
    "neuron1 = Neuron(test_input)\n",
    "out1 = neuron1.call()\n",
    "print(f'Output: {out1} | Shape: {out1.shape}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (50, 5)\n",
      "\n",
      "Neurons Shape: (20,)\n",
      "Nueron Input Shape: (50, 5)\n",
      "Weights Shape: (5,)\n",
      "Bias: 0.22086014221421513\n",
      "Dense1 Out: (50, 20)\n",
      "\n",
      "Neurons Shape: (20,)\n",
      "Nueron Input Shape: (50, 20)\n",
      "Weights Shape: (20,)\n",
      "Bias: 0.6765644532802416\n",
      "Dense2 Out: (50, 20)\n",
      "\n",
      "Neurons Shape: (1,)\n",
      "Nueron Input Shape: (50, 20)\n",
      "Weights Shape: (20,)\n",
      "Bias: 0.3544643930172282\n",
      "Output Layer: (50, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DenseLayer:\n",
    "    \n",
    "    def __init__(self, x, units, weights= -1, activation_func=linear_func, verbose=0):\n",
    "        \n",
    "        self.units = units\n",
    "        self.neurons: List[Neuron] = np.array([Neuron(x, weights, activation_func=activation_func, verbose=verbose) for _ in range(units)])\n",
    "        \n",
    "        if verbose == 1:\n",
    "            print(f'Neurons Shape: {self.neurons.shape}')\n",
    "            print(f'Nueron Input Shape: {self.neurons[0].x.shape}')\n",
    "            print(f'Weights Shape: {self.neurons[0].weights.shape}')\n",
    "            print(f'Bias: {self.neurons[0].bias}')\n",
    "        \n",
    "        \n",
    "    def call(self) -> np.ndarray:\n",
    "        \n",
    "        self.out = np.array([neuron.call() for neuron in self.neurons])\n",
    "        \n",
    "        return self.out.T\n",
    "        \n",
    "    \n",
    "print(f'Input Shape: {test_input.shape}\\n')\n",
    "\n",
    "dense1 = DenseLayer(test_input, units=20, activation_func=relu_func, verbose=1).call()\n",
    "print(f'Dense1 Out: {dense1.shape}\\n')\n",
    "\n",
    "dense2 = DenseLayer(dense1, units=20, activation_func=relu_func, verbose=1).call()\n",
    "print(f'Dense2 Out: {dense2.shape}\\n')\n",
    "\n",
    "out = DenseLayer(dense2, units=1, activation_func=sigmoid_func, verbose=1).call()\n",
    "print(f'Output Layer: {out.shape}\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: nan\n",
      "Weight Gradient: [inf inf]\n",
      "Bias Gradient: inf\n"
     ]
    }
   ],
   "source": [
    "X = jnp.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n",
    "y = jnp.array([0, 1, 1])\n",
    "w = jnp.array([0.1, 0.2])\n",
    "b = 0.5\n",
    "\n",
    "\n",
    "cost = compute_cost(X, y, w, b, activation_func=relu_func, loss_func=binary_cross_entropy_loss_func)\n",
    "w_diff_cost = grad(compute_cost, 2)\n",
    "b_diff_cost = grad(compute_cost, 3)\n",
    "\n",
    "\n",
    "print(\"Cost:\", cost)\n",
    "print(\"Weight Gradient:\", w_diff_cost(X, y, w, b, activation_func=relu_func, loss_func=binary_cross_entropy_loss_func))\n",
    "print(\"Bias Gradient:\", b_diff_cost(X, y, w, b, activation_func=relu_func, loss_func=binary_cross_entropy_loss_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([0, 0, 0, 1, 0])\n",
    "\n",
    "temp = test\n",
    "temp[temp <= 0] = 2\n",
    "\n",
    "print(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
